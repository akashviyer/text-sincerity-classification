# Text Sincerity Classification with BERT
This project utilizes BERT (Bidirectional Encoder Representations from Transformers), a pre-trained NLP model, to classify text for sincerity. The model underwent fine-tuning using Quora's dataset, which contains sincere and insincere questions. The impressive result is a validation accuracy of 96.35%. This project was great for my personal learning of NLP topics, transfer learning, and state-of-the-art NLP techniques.

## Project Overview
In this project, I aimed to determine whether text questions are sincere or insincere. I employed BERT, a powerful NLP model known for its contextual understanding of language.

## Model Pretraining
BERT, short for Bidirectional Encoder Representations from Transformers, was pre-trained on a massive corpus of text data. This pretraining process gave the model a deep understanding of language structure, making it an ideal candidate for various NLP tasks.

## Fine Tuning
To make BERT suitable for sincerity classification, I fine-tuned the model using a dataset from Quora. This dataset consists of questions categorized as either sincere or insincere. The fine-tuning process adapted BERT to this specific task.

## Results
The model achieved remarkable performance during validation, with an accuracy of 96.35%. This high accuracy indicates its ability to distinguish between sincere and insincere questions effectively.

Feel free to explore the project further to understand its implementation and the potential applications of this sincerity classification model.
